{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3773,
     "status": "ok",
     "timestamp": 1670230089531,
     "user": {
      "displayName": "Htet Htet Aung",
      "userId": "12234662517384541802"
     },
     "user_tz": -60
    },
    "id": "o7Wji5FACz6t",
    "outputId": "127ee641-f170-4d6d-92ff-016311b95119"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/htet/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/htet/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/htet/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/htet/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I never vomited from this food but after start...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tried to change over to this brand, and my mom...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>My children love this food. Gives them the die...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Recommended, Nice Product, Fast Delivery. Wort...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text Sentiment\n",
       "0  I never vomited from this food but after start...  negative\n",
       "1  Tried to change over to this brand, and my mom...  negative\n",
       "2  My children love this food. Gives them the die...  positive\n",
       "3  Recommended, Nice Product, Fast Delivery. Wort...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_dict = {\n",
    "    \"Text\": [\"I never vomited from this food but after started this brand, I was vomiting twice a day. A waste of money and disappointing. Cannot even get a refund for it.\",\n",
    "               \"Tried to change over to this brand, and my mom just doesn't like it and won't eat it.\",\n",
    "               \"My children love this food. Gives them the diet. I canâ€™t personally attest to the flavor since I have not eaten any.\",\n",
    "               \"Recommended, Nice Product, Fast Delivery. Worth for money. The ingredients are of good quality, very fresh.\"],\n",
    "    \"Sentiment\": [\"negative\", \"negative\", \"positive\", \"positive\"]\n",
    "}\n",
    "\n",
    "reviews = pd.DataFrame(reviews_dict)\n",
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i never vomited from this food but after started this brand, i was vomiting twice a day. a waste of money and disappointing. cannot even get a refund for it.\n",
      "\n",
      "***Reviews after removing stopwords and lemmating the words***\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'never vomit food start brand , vomit twice day . waste money disappoint . even get refund .'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert words to lower case\n",
    "reviews['Text'] = reviews['Text'].str.lower()\n",
    "\n",
    "# Remove special characters \n",
    "reviews['Text'] = reviews['Text'].str.replace(r'[^A-Za-z0-9 ]+', '')\n",
    "print(reviews['Text'][0])     \n",
    "\n",
    "# Remove stopwords and lemmatize the words\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "reviews['Text'] = reviews['Text'].apply(lambda review: ' '.join([lemmatizer.lemmatize(word, pos=\"v\") for word in word_tokenize(review) if word not in stop_words]))\n",
    "print(\"\\n***Reviews after removing stopwords and lemmating the words***\")\n",
    "reviews['Text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF model to vectorize input data\n",
    "tfidf_path = \"/home/htet/Documents/backup/sentiment_analysis_of_amazon_product_reviews/final_model/model_tfidf.pkl\"\n",
    "\n",
    "# Load the model\n",
    "with open(tfidf_path, 'rb') as f:\n",
    "    tfidf = pickle.load(f)\n",
    "\n",
    "# Transform the data\n",
    "review_tfidf = tfidf.transform(reviews['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize matrices\n",
    "scaler = MaxAbsScaler()\n",
    "review_norm = scaler.fit_transform(review_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictied Sentiments: [0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Predict the sentiment\n",
    "model_path = \"/home/htet/Documents/backup/sentiment_analysis_of_amazon_product_reviews/final_model/model_lsvm.pkl\"\n",
    "\n",
    "with open(model_path, 'rb') as f:\n",
    "    rfc = pickle.load(f)\n",
    "    \n",
    "prediction = rfc.predict(review_norm)\n",
    "print('Predictied Sentiments:', prediction)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMpAl4gH6qhomd+2+sHxzeK",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
